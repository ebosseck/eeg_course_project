{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568c70ac9cd0d450",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Group: Hyacithara - Report\n",
    "\n",
    "## Introductory Notes\n",
    "\n",
    "This project including Jupyter notebooks and additional source code was created as submission to the EEG course in winter 2023/24.\n",
    "This notebook `01_prepare_data.ipynb` serves as guide and interactive installation and setup software.\n",
    "The pipeline gets run in the subsequent notebook `02_run_pipeline.ipynb`.\n",
    "\n",
    "To run this notebook, matching software including Python and e. g. Jupyter Lab need to be installed.\n",
    "\n",
    "For a better overview, in Jupyter Lab, the Table of Contents tab can get opened in the left column.\n",
    "Since the entries are interactive, it automatically updated and a click on the title initiates a jump to the section.\n",
    "\n",
    "## System and Dependencies\n",
    "\n",
    "All operations were tested on computers with 64 Bit multi-core processor and at least 16 GB RAM.\n",
    "As operating systems, Microsoft Windows 10, Microsoft Windows 11, Manjaro Linux 23.1.3, and Fedora Linux 39 were used.\n",
    "On all computers, Python 3.10 or 3.11 - e. g. Python 3.11.8 - was installed for running `pip` and the pipeline.\n",
    "\n",
    "To make sure that all required Python packages are available, the installation gets started in the following cell.\n",
    "\n",
    "- `!` allows for running the following command on the command line.\n",
    "- `pip install -r <textfile>` (or depending on the installation `python3 -m pip …`) installs the module dependencies in the versions with which this notebook was created.\n",
    "- With these module versions combined with Python 3.11.8, the notebook was tested and running. With other versions, errors can occur. One example of a problem caused by non-matching versions is the changed step naming of different `mne-bids-pipeline` versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1a44d77400139",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt # install dependencies\n",
    "%env PYTHONIOENCODING=utf8 # set modern encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e3b8b3c55a98d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Download and Pre-process the Dataset\n",
    "\n",
    "Now, that all required Python packages are installed, we can start to fetch all required data and perform some clean-up operations on the data.\n",
    "Then, we can use the data with the MNE-BIDS pipeline.\n",
    "\n",
    "### Preamble\n",
    "To perform the pre-processing, the custom Python module gets added to the search path.\n",
    "Then, the needed modules for loading the configuration file and for fetching the dataset get loaded.\n",
    "\n",
    "To allow for plotting, a plotting library with API similar to the one of Matlab gets loaded.\n",
    "This then is set to use a QT-based rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project source code directory to the search path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./src/'))\n",
    "\n",
    "# import function to load configuration from file\n",
    "from mne_bids_pipeline._config_import import _import_config as getConfig\n",
    "from tools.logtools import *\n",
    "\n",
    "# tools to get fresh data\n",
    "import data_handling.data_downloader as dl\n",
    "import data_handling.data_cleaner as clean\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('qtagg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e700f72b77cb5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Load Configuration\n",
    "\n",
    "First of all, the configuration for the MNE BIDS pipeline gets loaded from the prepared file.\n",
    "Note, that file existence checks are disabled here.\n",
    "Otherwise, the import would fail, if the data are not yet available.\n",
    "Therefore, in the first run of this notebook, loading the configuration would fail.\n",
    "\n",
    "In the Jupyter notebook for running the pipeline, short checkups get performed.\n",
    "These show few data written to the configuration, if the file gets properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8ac0a1562327c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set the file path of the main configuration file\n",
    "bids_config_path = \"./mne-bids/config/mne-bids-pipeline.py\"\n",
    "# load configured settings from file\n",
    "bids_cfg = getConfig(\n",
    "    config_path=bids_config_path,\n",
    "    check=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0067e01f3b48c6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dataset Download\n",
    "\n",
    "After loading the custom data handling module, a check for the dataset existence is started.\n",
    "For ensuring to have an unchanged version of the dataset, the fresh download can get enforced.\n",
    "\n",
    "If the dataset does not exist in the location specified in the configuration file, a copy gets downloaded and extracted.\n",
    "Please ensure that you have enough disk space available. About 130 GB are needed for download and extraction. Additional 40 GB should be free for running the pipeline, which can be freed by deleting the file `ds003702.zip` after successful extraction.\n",
    "\n",
    "For existence checkup and download, there are some configuration options in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b28eb3a3f246b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from data_handling import getDataPathFromBidsRoot\n",
    "\n",
    "dl.CLEAN_DATA = False # if true, clears the data directory in order to force downloading a fresh copy of the data\n",
    "dl.DATA_BASE_DIR = getDataPathFromBidsRoot(bids_cfg.bids_root) # get the data folder from the bids pipeline configuration\n",
    "dl.VALIDATE_DATA = True # if true, checks that the downloaded zip file is the expected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3e8883be7ee99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dl.CLEAN_DATA:\n",
    "    clean.removeDirectory(dl.DATA_BASE_DIR)\n",
    "dl.fetchData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68769931b45f518",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Pre-processing of Data Files\n",
    "\n",
    "Once all data is downloaded and unpacked, the format and content of multiple contained files needs to get updated.\n",
    "This allows for direct use of the updated dataset with tools of MNE BIDS pipeline.\n",
    "\n",
    "For this data set, this consists mainly of two tasks:\n",
    "\n",
    "1. Fix file links in `*.vhdr` and `*.vmrk` files. This is needed, because the files got renamed after exporting, but the original authors did not fix the file links\n",
    "2. Generate a `*_events.tsv` file containing for each subject, which contains onset time, duration, and type for each labelled time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ddf79dffd021e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import tools to patch fresh data\n",
    "import data_handling.data_patcher as patch\n",
    "import data_handling.convert_brainvision2bids as convert\n",
    "\n",
    "# run patches\n",
    "patch.patchAllFiles(bids_cfg.bids_root)\n",
    "convert.buildEventTSV(bids_cfg.bids_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfd512c3fba5ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Validity Check\n",
    "\n",
    "Now that we got all the data we require, we can import the config again.\n",
    "This time, it is done with checks for all parameters being valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3088771f4cffa8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bids_cfg = getConfig(\n",
    "    config_path=bids_config_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2acedeb38ffb903",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Electrode Coordinates\n",
    "Next, we look at the used electrode coordinates.\n",
    "The authors have chosen the 1010 system.\n",
    "Since this is not directly given, we chose to load the coordinates of the 1005 system electrodes insdead of defining a custom 1010 system.\n",
    "\n",
    "The unused positions get ignored.\n",
    "The electrodes, for which recorded signals are given, are virtually positioned at the correct positions in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb02521fb594da7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Once the preparatory steps are done and a configuration is loaded, the pipeline can get run.\n",
    "\n",
    "In this notebook, the initial setup of the pipeline output gets run.\n",
    "These steps include loading modules as dependencies, optionally resetting the output data directory, and - if empty - initialising the output directory.\n",
    "\n",
    "### Load Dependencies\n",
    "\n",
    "In addition to the modules loaded as dependencies in the next cell, some dependencies were loaded in a previous cell.\n",
    "This includes `mne-bids-pipeline`, which is in use for loading the configuration from the prepared file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811e39d8ee675e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# allow for calling mne_bids_pipeline within Python\n",
    "import sys\n",
    "\n",
    "from mne_bids import BIDSPath\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113709b71277e18e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Deletion of Prior Outputs (Optional)\n",
    "\n",
    "In case errors occur while running the pipeline, we remove the output of the previous pipeline runs.\n",
    "This gets done by running the following two cells after setting `CLEAR_PIPELINE_OUTPUT` to `True`.\n",
    "\n",
    "Note: If the value is set to `True`, the computations of the pipeline will need more time than when using some pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fe23fbac4c448",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CLEAR_PIPELINE_OUTPUT = False # False: Keep previous pipeline output; True: Delete all previous pipeline outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e970eb4371de4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if CLEAR_PIPELINE_OUTPUT:\n",
    "    clean.removeDirectory(\"{}/derivatives/mne-bids-pipeline\".format(bids_cfg.bids_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d7feb115806f7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Pipeline: Initial Pipeline Run\n",
    "\n",
    "At this step, the preparations are finished.\n",
    "Therefore, we can start running the pipeline based on the configuration file.\n",
    "\n",
    "The initialisation should create e. g. needed directories for the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b85f17c1e50329",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"init\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97841505b3f049b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In case of Unicode encode errors when attempting to run the pipeline, make sure that the following environment variable is set:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "198744ccc077d5a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "PYTHONIOENCODING=utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922764c372c2fcb8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The environment variable should already be set, if the first cell - the cell containing the module installation via `pip` - was run after starting the current Python kernel.\n",
    "\n",
    "Remember to restart Jupyter after setting the environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b95c04-344c-4e40-8df3-838faaf1069a",
   "metadata": {},
   "source": [
    "### Pipeline: Pre-processing and Analysis\n",
    "\n",
    "The following steps get run in the next Jupyter notebook:\n",
    "```\n",
    "02_run_pipeline.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4e0d4-8656-48e4-ae22-fbb1115c5381",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3024b9b-a3b5-4318-87b3-ee865c347fd0",
   "metadata": {},
   "source": [
    "---\n",
    "After the initialisation, the first pre-processing steps working on the measured eeg signals get applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2249213",
   "metadata": {},
   "source": [
    "For the independent component analysis, we use the extended_infomax algorithm.\n",
    "\n",
    "We apply a first peak-to-peak rejection to prevent high-power noise signals from influencing the ICA.\n",
    "mne-bids requires us to specify a fixed threshold, for which we struggled to find a good value:\n",
    "We went with 400 micro volt, but for some subjects this will reject all epochs, leading to the entire subject being exluded.\n",
    "(TODO: take a closer look at those subjects, maybe there's a different issue at core?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e2ee257bc36ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_06a_run_ica\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef874c",
   "metadata": {},
   "source": [
    "(TODO: Put this into a separate utils file?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe074dfbe68acf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from mne_bids import BIDSPath\n",
    "# define a function which gets used in application of the ICA results to the raw data\n",
    "def get_input_fnames_apply_ica(\n",
    "        *,\n",
    "        cfg,\n",
    "        subject: str,\n",
    "        session: Optional[str],\n",
    ") -> dict:\n",
    "    bids_basename = BIDSPath(\n",
    "        subject=subject,\n",
    "        session=session,\n",
    "        task=cfg.task,\n",
    "        acquisition=cfg.acq,\n",
    "        recording=cfg.rec,\n",
    "        space=cfg.space,\n",
    "        datatype='eeg',\n",
    "        root=cfg.deriv_root,\n",
    "        check=False,\n",
    "    )\n",
    "    paths = dict()\n",
    "    paths[\"ica\"] = bids_basename.copy().update(suffix=\"ica\", extension=\".fif\")\n",
    "    paths[\"raw\"] = bids_basename.copy().update(suffix=\"proc-filt_raw\", extension=\".fif\")\n",
    "    paths[\"components\"] = bids_basename.copy().update(\n",
    "        processing=\"ica\", suffix=\"components\", extension=\".tsv\"\n",
    "    )\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55642d",
   "metadata": {},
   "source": [
    "Here, we automatically classify the components using iclabel.\n",
    "We keep components labeled as \"brain\", as well as those labeled as \"other\", as this usually means that there is not enough information to clearly exlude them.\n",
    "Anything labeled differently (e.g. muscle artifact, eye blink, channel noise, ...) is marked as \"bad\" in the .tsv file and will thus be excluded in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334c87f1e2b46f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import mne\n",
    "import mne_icalabel\n",
    "from mne.preprocessing import read_ica\n",
    "import pandas as pd\n",
    "from mne_bids_pipeline._config_utils import (\n",
    "    get_subjects,\n",
    "    get_sessions\n",
    ")\n",
    "\n",
    "for subject in get_subjects(bids_cfg):\n",
    "    for session in get_sessions(bids_cfg):\n",
    "        paths = get_input_fnames_apply_ica(cfg=bids_cfg, subject=subject, session=session)\n",
    "        if not exists(paths[\"ica\"]):\n",
    "            print(formatString(\"ICA file not found, skipping Subject:\", style=STYLE_TEXT_RED),\n",
    "                  formatString(subject, style=STYLE_TEXT_BLUE))\n",
    "            continue\n",
    "        ica = read_ica(paths[\"ica\"])\n",
    "        raw = mne.io.read_raw_fif(paths[\"raw\"])\n",
    "\n",
    "        label_results = mne_icalabel.label_components(raw, ica, method=\"iclabel\")\n",
    "\n",
    "        print(str(ica))  # checkup print of known data about ICA\n",
    "        print(\"\\nresulting predictions:\", label_results[\"y_pred_proba\"])  # checkup print\n",
    "        print(\"\\nresulting labels:     \", label_results[\"labels\"])  # checkup print\n",
    "\n",
    "        labels = label_results[\"labels\"]\n",
    "        exclude_idx = [\n",
    "            idx for idx, label in enumerate(labels) if label not in [\"brain\", \"other\"]\n",
    "        ]\n",
    "        tsv_data = pd.read_csv(paths[\"components\"], sep=\"\\t\")\n",
    "\n",
    "        # checkup: print old content of the file\n",
    "        print(\"\\nold tsv file content:\")\n",
    "        print(str(tsv_data))\n",
    "\n",
    "        tsv_data.loc[exclude_idx, \"status\"] = \"bad\"\n",
    "\n",
    "        # checkup: print updated content of the file\n",
    "        print(\"\\nnew tsv file content:\")\n",
    "        print(tsv_data)\n",
    "\n",
    "        tsv_data.to_csv(paths[\"components\"], sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ba36f",
   "metadata": {},
   "source": [
    "Here we apply the decisions from iclabel from above, rejecting any \"bad\" components. Before this, we might want to take a look at the components ourselves, to see if we agree with the automated classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f8a79e11414d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_07a_apply_ica\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f73b7fd53ba77d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_08_ptp_reject\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf33ffd3c56704",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"sensor\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb194a29cf5eb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"source\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
