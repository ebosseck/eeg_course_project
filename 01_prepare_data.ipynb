{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568c70ac9cd0d450",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Group: Hyacithara - Report\n",
    "\n",
    "## System and Dependencies\n",
    "\n",
    "All operations were tested on computers with 64 Bit multi-core processor and at least 16 GB RAM.\n",
    "As operating systems, Microsoft Windows 10, Microsoft Windows 11, Manjaro Linux 23.1.3, and Fedora Linux 39 were used.\n",
    "On all computers, Python 3.10 or 3.11 - e. g. Python 3.11.8 - was installed for running `pip` and the pipeline.\n",
    "\n",
    "To make sure that all required Python packages are available, the installation gets started in the following cell.\n",
    "\n",
    "- `!` allows for running the following command on the command line.\n",
    "- `pip install -r <textfile>` (or depending on the installation `python3 -m pip …`) installs the module dependencies in the versions with which this notebook was created.\n",
    "- With these module versions combined with Python 3.11.8, the notebook was tested and running. With other versions, errors can occur. One example of a problem caused by non-matching versions is the changed step naming of different `mne-bids-pipeline` versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1a44d77400139",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt # install dependencies\n",
    "%env PYTHONIOENCODING=utf8 # set modern encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e3b8b3c55a98d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Download and Pre-process the Dataset\n",
    "\n",
    "Now, that all required Python packages are installed, we can start to fetch all required data and perform some clean-up operations on the data.\n",
    "Then, we can use the data with the MNE-BIDS pipeline.\n",
    "\n",
    "### Preamble\n",
    "To perform the pre-processing, the custom Python module gets added to the search path.\n",
    "Then, the needed modules for loading the configuration file and for fetching the dataset get loaded.\n",
    "\n",
    "To allow for plotting, a plotting library with API similar to the one of Matlab gets loaded.\n",
    "This then is set to use a QT-based rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project source code directory to the search path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./src/'))\n",
    "\n",
    "# import function to load configuration from file\n",
    "from mne_bids_pipeline._config_import import _import_config as getConfig\n",
    "from tools.logtools import *\n",
    "\n",
    "# tools to get fresh data\n",
    "import data_handling.data_downloader as dl\n",
    "import data_handling.data_cleaner as clean\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('qtagg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e700f72b77cb5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Load Configuration\n",
    "\n",
    "First of all, we load the configuration for the mne-bids pipeline from the prepared file.\n",
    "Note, that we have to disable checks here.\n",
    "Otherwise, the import would fail if the data is not yet available like in the first run of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8ac0a1562327c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set the file path of the main configuration file\n",
    "bids_config_path = \"./mne-bids/config/mne-bids-pipeline_data_tests.py\"\n",
    "# load configured settings from file\n",
    "bids_cfg = getConfig(\n",
    "    config_path=bids_config_path,\n",
    "    check=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0067e01f3b48c6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dataset Download\n",
    "\n",
    "After loading the custom data handling module, a check for the dataset existence is started.\n",
    "For ensuring to have an unchanged version of the dataset, the fresh download can get enforced.\n",
    "\n",
    "If the dataset does not exist in the location specified in the configuration file, a copy gets downloaded and extracted.\n",
    "Please ensure that you have enough disk space available. About 130 GB are needed for download and extraction. Additional 40 GB should be free for running the pipeline, which can be freed by deleting the file `ds003702.zip` after successful extraction.\n",
    "\n",
    "For existence checkup and download, there are some configuration options in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b28eb3a3f246b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from data_handling import getDataPathFromBidsRoot\n",
    "\n",
    "dl.CLEAN_DATA = False # if true, clears the data directory in order to force downloading a fresh copy of the data\n",
    "dl.DATA_BASE_DIR = getDataPathFromBidsRoot(bids_cfg.bids_root) # get the data folder from the bids pipeline configuration\n",
    "dl.VALIDATE_DATA = True # if true, checks that the downloaded zip file is the expected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3e8883be7ee99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if dl.CLEAN_DATA:\n",
    "    clean.removeDirectory(dl.DATA_BASE_DIR)\n",
    "dl.fetchData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68769931b45f518",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "Once all data is downloaded and unpacked, we have to do some preprocessing, in order to be able to use the data.\n",
    "\n",
    "For this data set, this consists mainly of two tasks:\n",
    "\n",
    "1. Fix file links in `*.vhdr` and `*.vmrk` files. This is needed, because the files got renamed after exporting, but the original authors did not fix the file links\n",
    "2. Generate a `*_events.tsv` file containing for each subject, which contains onset time, duration, and type for each labelled time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ddf79dffd021e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import tools to patch fresh data\n",
    "import data_handling.data_patcher as patch\n",
    "import data_handling.convert_brainvision2bids as convert\n",
    "\n",
    "patch.patchAllFiles(bids_cfg.bids_root)\n",
    "convert.buildEventTSV(bids_cfg.bids_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfd512c3fba5ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Validity Check\n",
    "\n",
    "Now that we got all the data we require, we can import the config again.\n",
    "This time, it is done with checks for all parameters being valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3088771f4cffa8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bids_cfg = getConfig(\n",
    "    config_path=bids_config_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2acedeb38ffb903",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Electrode Coordinates\n",
    "Next, we look at the used electrode coordinates.\n",
    "The authors have chosen the 1010 system.\n",
    "Since this is not directly given, we chose to load the coordinates of the 1005 system electrodes insdead of defining a custom 1010 system.\n",
    "\n",
    "The unused positions get ignored.\n",
    "The electrodes, for which recorded signals are given, are virtually positioned at the correct positions in the pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d56d4f16-4673-4579-8bed-5dfed760ba40",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from tools.create1010montage import create1010\n",
    "create1010()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0cef3ea-3fea-4073-aa29-3a0be90ef4b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import mne\n",
    "\n",
    "mne.channels.get_builtin_montages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb02521fb594da7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Once the preparatory steps are done, and a configuration is loaded, we can run the pipeline.\n",
    "\n",
    "### Load Dependencies\n",
    "\n",
    "In addition to the modules loaded as dependencies in the next cell, some dependencies were loaded in a previous cell.\n",
    "This includes `mne-bids-pipeline`, which is in use for loading the configuration from the prepared file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811e39d8ee675e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# allow for calling mne_bids_pipeline within Python\n",
    "import sys\n",
    "\n",
    "from mne_bids import BIDSPath\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113709b71277e18e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Deletion of Prior Outputs (Optional)\n",
    "\n",
    "In case errors occur while running the pipeline, we remove the output of the previous pipeline runs.\n",
    "This gets done by running the following two cells after setting `CLEAR_PIPELINE_OUTPUT` to `True`.\n",
    "\n",
    "Note: If the value is set to `True`, the computations of the pipeline will need more time than when using some pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fe23fbac4c448",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CLEAR_PIPELINE_OUTPUT = False # False: Keep previous pipeline output; True: Delete all previous pipeline outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e970eb4371de4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if CLEAR_PIPELINE_OUTPUT:\n",
    "    clean.removeDirectory(\"{}/derivatives/mne-bids-pipeline\".format(bids_cfg.bids_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d7feb115806f7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Pipeline: Initial Pipeline Run\n",
    "\n",
    "At this step, the preparations are finished.\n",
    "Therefore, we can start running the pipeline based on the configuration file.\n",
    "\n",
    "The initialisation should create e. g. needed directories for the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b85f17c1e50329",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"init\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97841505b3f049b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In case of Unicode encode errors when attempting to run the pipeline, make sure that the following environment variable is set:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "198744ccc077d5a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "PYTHONIOENCODING=utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922764c372c2fcb8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The environment variable should already be set, if the first cell - the cell containing the module installation via `pip` - was run after starting the current Python kernel.\n",
    "\n",
    "Remember to restart Jupyter after setting the environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b95c04-344c-4e40-8df3-838faaf1069a",
   "metadata": {},
   "source": [
    "### Pipeline: Pre-processing\n",
    "\n",
    "After the initialisation, the first pre-processing steps working on the measured eeg signals get applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d272040fb76aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_01_data_quality\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b975a907876614",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_02_head_pos\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc7c8be65056ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_03_maxfilter\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05edb2a1",
   "metadata": {},
   "source": [
    "We apply a bandpass filter with a highpass frequency of 1.0 Hz and a lowpass frequency of 30.0 Hz.\n",
    "The lowpass filter removes most high frequency noise from the data, including D/C noise. \n",
    "The highpass filter does some implicit detrending, and is a good practice in preparation for the ICA artefact removal.\n",
    "\n",
    "We decided for such a narrow filter band, as our analysis is based on more narrow bandwidths.\n",
    "These frequency bands in our analysis are alpha and theta oscillations.\n",
    "Both bands cutoff frequencies are in the range of 3 Hz to 12 Hz. \n",
    "\n",
    "Thus, removing the energy of frequency bands below 1 Hz and above 30 Hz should not remove relevant information for this analysis.\n",
    "Furthermore, the lowpass frequency of 30 Hz allows for downsampling the signals with sample rates of at least 60 Hz.\n",
    "\n",
    "This is based on the Nyquist theoreme: The sample rate should be at least twice as high as the highest frequency given in the signal.\n",
    "If this is not fulfilled, by aliasing the energy of the signal at frequencies higher than the Nyquist frequency gets added to the downsampled signal at other frequencies.\n",
    "\n",
    "Since the filter curve is expected to be non-perfect in the sense of implementing a step from 0 dB amplification to -∞ dB at the cutoff frequency, a decreasing energy over the frequency above the cutoff frequency is expected.\n",
    "Based on this energy decay over the frequency, we chose a higher sample rate of 100 Hz to get a slightly improved signal to noise ratio compared to a lower sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa43da11ab7d2b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_04_frequency_filter\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a6b34",
   "metadata": {},
   "source": [
    "We can now check the power spectrum from one of the subjects to see the effect of the filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbd2d54f7d6424",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "# checkup: print spectrum of original and filtered signals, print sample rate of new raw data file\n",
    "curr_checkup_filename = f\"{bids_cfg.bids_root}/derivatives/mne-bids-pipeline/sub-01/eeg/sub-01_task-SocialMemoryCuing_proc-filt_raw.fif\"\n",
    "curr_checkup_raw_filtered = mne.io.read_raw_fif(curr_checkup_filename, preload=True)\n",
    "curr_checkup_raw_filtered.plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903a517",
   "metadata": {},
   "source": [
    "Next, the epochs are being created:\n",
    "We start the epochs one second before the cue object appears and end them seven seconds later, thus the time interval being -1.0 - 6.0 seconds.\n",
    "Such an epoch will include a baseline of 1 second (-1s to 0s), the cue pointing down (0s to 1.5s), the cue shifting up (1.5s to 2s), the cue \"looking\" at the subject directly (2s to 3s), the cue shifting left or right (3s to 3.5s), the memory targets being shown (3.5s to 4s), a maintenance interval (4s to 5s), and one last second where the subject is shown a location target (5s to 6s).\n",
    "(TODO: reconsider if that last second is relevant at all, maybe we could cut off after maintenance interval since we don't really know what happens during 5s and 6s: target might respond quickly, seeing feedback, or only respond after interval is over, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac3904cce52ab4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_05_make_epochs\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2249213",
   "metadata": {},
   "source": [
    "For the independent component analysis, we use the extended_infomax algorithm.\n",
    "\n",
    "We apply a first peak-to-peak rejection to prevent high-power noise signals from influencing the ICA.\n",
    "mne-bids requires us to specify a fixed threshold, for which we struggled to find a good value:\n",
    "We went with 400 micro volt, but for some subjects this will reject all epochs, leading to the entire subject being exluded.\n",
    "(TODO: take a closer look at those subjects, maybe there's a different issue at core?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e2ee257bc36ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_06a_run_ica\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef874c",
   "metadata": {},
   "source": [
    "(TODO: Put this into a separate utils file?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe074dfbe68acf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from mne_bids import BIDSPath\n",
    "# define a function which gets used in application of the ICA results to the raw data\n",
    "def get_input_fnames_apply_ica(\n",
    "        *,\n",
    "        cfg,\n",
    "        subject: str,\n",
    "        session: Optional[str],\n",
    ") -> dict:\n",
    "    bids_basename = BIDSPath(\n",
    "        subject=subject,\n",
    "        session=session,\n",
    "        task=cfg.task,\n",
    "        acquisition=cfg.acq,\n",
    "        recording=cfg.rec,\n",
    "        space=cfg.space,\n",
    "        datatype='eeg',\n",
    "        root=cfg.deriv_root,\n",
    "        check=False,\n",
    "    )\n",
    "    paths = dict()\n",
    "    paths[\"ica\"] = bids_basename.copy().update(suffix=\"ica\", extension=\".fif\")\n",
    "    paths[\"raw\"] = bids_basename.copy().update(suffix=\"proc-filt_raw\", extension=\".fif\")\n",
    "    paths[\"components\"] = bids_basename.copy().update(\n",
    "        processing=\"ica\", suffix=\"components\", extension=\".tsv\"\n",
    "    )\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55642d",
   "metadata": {},
   "source": [
    "Here, we automatically classify the components using iclabel.\n",
    "We keep components labeled as \"brain\", as well as those labeled as \"other\", as this usually means that there is not enough information to clearly exlude them.\n",
    "Anything labeled differently (e.g. muscle artifact, eye blink, channel noise, ...) is marked as \"bad\" in the .tsv file and will thus be excluded in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334c87f1e2b46f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import mne\n",
    "import mne_icalabel\n",
    "from mne.preprocessing import read_ica\n",
    "import pandas as pd\n",
    "from mne_bids_pipeline._config_utils import (\n",
    "    get_subjects,\n",
    "    get_sessions\n",
    ")\n",
    "\n",
    "for subject in get_subjects(bids_cfg):\n",
    "    for session in get_sessions(bids_cfg):\n",
    "        paths = get_input_fnames_apply_ica(cfg=bids_cfg, subject=subject, session=session)\n",
    "        if not exists(paths[\"ica\"]):\n",
    "            print(formatString(\"ICA file not found, skipping Subject:\", style=STYLE_TEXT_RED),\n",
    "                  formatString(subject, style=STYLE_TEXT_BLUE))\n",
    "            continue\n",
    "        ica = read_ica(paths[\"ica\"])\n",
    "        raw = mne.io.read_raw_fif(paths[\"raw\"])\n",
    "\n",
    "        label_results = mne_icalabel.label_components(raw, ica, method=\"iclabel\")\n",
    "\n",
    "        print(str(ica))  # checkup print of known data about ICA\n",
    "        print(\"\\nresulting predictions:\", label_results[\"y_pred_proba\"])  # checkup print\n",
    "        print(\"\\nresulting labels:     \", label_results[\"labels\"])  # checkup print\n",
    "\n",
    "        labels = label_results[\"labels\"]\n",
    "        exclude_idx = [\n",
    "            idx for idx, label in enumerate(labels) if label not in [\"brain\", \"other\"]\n",
    "        ]\n",
    "        tsv_data = pd.read_csv(paths[\"components\"], sep=\"\\t\")\n",
    "\n",
    "        # checkup: print old content of the file\n",
    "        print(\"\\nold tsv file content:\")\n",
    "        print(str(tsv_data))\n",
    "\n",
    "        tsv_data.loc[exclude_idx, \"status\"] = \"bad\"\n",
    "\n",
    "        # checkup: print updated content of the file\n",
    "        print(\"\\nnew tsv file content:\")\n",
    "        print(tsv_data)\n",
    "\n",
    "        tsv_data.to_csv(paths[\"components\"], sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ba36f",
   "metadata": {},
   "source": [
    "Here we apply the decisions from iclabel from above, rejecting any \"bad\" components. Before this, we might want to take a look at the components ourselves, to see if we agree with the automated classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f8a79e11414d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_07a_apply_ica\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f73b7fd53ba77d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"preprocessing/_08_ptp_reject\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf33ffd3c56704",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"sensor\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb194a29cf5eb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "curr_steps = \"source\"\n",
    "!mne_bids_pipeline --config {bids_config_path} --steps {curr_steps}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
